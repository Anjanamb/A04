import tensorflow as tf
from tensorflow import keras
import numpy as np
import matplotlib.pyplot as plt

(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()
print('x_train: ', x_train.shape)

K = len(np.unique(y_train)) # Classes
Ntr = x_train.shape[0]
Nte = x_test.shape[0]
Din = 3072 # CIFAR10
# Din = 784 # MINIST

# Normalize pixel values
#x_train, x_test = x_train / 255.0, x_test / 255.0
mean_image = np.mean(x_train, axis=0)
x_train = x_train - mean_image
x_test = x_test - mean_image

y_train = tf.keras.utils.to_categorical(y_train, num_classes=K)
y_test = tf.keras.utils.to_categorical(y_test, num_classes=K)

# Transform images from (width, width, 3) to 3072-dimensional vectors (width*width*3)
x_train = np.reshape(x_train,(Ntr,Din))
x_test = np.reshape(x_test,(Nte,Din))
x_train = x_train.astype('float32')
x_test = x_test.astype('float32')

H = 200
std = 1e-5
w1 = std*np.random.randn(Din, H)
w2 = std*np.random.randn(H, K)
b1 = np.zeros(H)
b2 = np.zeros(K)
print("w1:", w1.shape)
print("b1:", b1.shape)
print("w2:", w2.shape)
print("b2:", b2.shape)
batch_size = 500

iterations = 300 # epochs
lr = 0.0009 # learning rate
lr_decay = 0.99999
reg = 0.004
loss_history = []
train_acc_history = []
val_acc_history = []
#seed = 0
#rng = np.random.default_rng(seed=seed)
for t in range(iterations):
    #batch_indices = np.arange(batch_size)
    #rng.shuffle(batch_indices)
    batch_indices = np.random.choice(Ntr, batch_size)
    x = x_train[batch_indices]
    y = y_train[batch_indices]
    h = 1.0/(1.0 + np.exp(-(x.dot(w1) + b1)))
    y_pred = h.dot(w2) +b2
    loss = 1./batch_size*np.square(y_pred - y).sum() + reg * (np.sum(w2 * w2) + np.sum(w1 * w1))
    loss_history.append(loss)
    if t % 10 == 0:
      print('Iteration %d / %d: loss %f' % (t, iterations, loss))

    dy_pred = 1./batch_size*2.0*(y_pred -y) # partial derivative of L w.r.t y_hat backward
    dw2 = h.T.dot(dy_pred) + reg*w2
    db2 = dy_pred.sum(axis=0)
    dh = dy_pred.dot(w2.T)
    dw1 = x.T.dot(dh*h*(1-h)) + reg*w1
    db1 = (dh*h*(1-h)).sum(axis=0)
    w1 -= lr*dw1
    w2 -= lr*dw2
    b1 -= lr*db1
    b2 -= lr*db2
    lr *= lr_decay
 
plt.plot(loss_history)
plt.xlabel('Epocs')
plt.ylabel('Loss')
f, axarr = plt.subplots(1,10)
f.set_size_inches(16, 6)

for i in range(10):
    img = w1[:, i].reshape(32, 32, 3) # CIFAR10
    #img = w1[:, i].reshape(28, 28) # MNIST
    axarr[i].imshow(img*1000)
plt.show()

x_tr = x_train
print("x_train ->", x_tr.shape)
h = 1.0/(1.0 + np.exp(-(x_tr.dot(w1) + b1))) #sigmoid
y_pred = h.dot(w2) + b2

tr_acc = 1.0 -1/(Ntr*9)*(np.abs(np.argmax(y_train,axis=1) - np.argmax(y_pred, axis=1))).sum()
tr_loss = 1./Ntr*np.square(y_pred-y_train).sum() + reg * (np.sum(w2 * w2) + np.sum(w1 * w1))
print("train_acc = ", tr_acc)
print("train_loss = ", tr_loss)

x_ts = x_test
print("x_test ->", x_ts.shape)
h = 1.0/(1.0 + np.exp(-(x_ts.dot(w1) + b1))) #sigmoid
y_pred= h.dot(w2) + b2

ts_acc = 1.0 - 1/(Nte*9)*(np.abs(np.argmax(y_test,axis=1) - np.argmax(y_pred, axis=1))).sum()
ts_loss = 1./Nte*np.square(y_pred-y_test).sum() + reg * (np.sum(w2 * w2) + np.sum(w1 * w1))
print("test_acc = ", ts_acc)
print("test_loss = ", ts_loss)
